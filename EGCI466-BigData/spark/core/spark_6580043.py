# -*- coding: utf-8 -*-
"""Spark 6580043.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ac0yX2syhBqMhxVBV9YkOXDUmpjsqAxB
"""

# Installing required packages
#!pip install pyspark
#!pip install findspark

#import findspark
#findspark.init()

from pyspark import SparkContext,SparkConf
# Creating a spark context class
sc = SparkContext()



'''
from google.colab import drive
drive.mount('/content/drive')
file=open('/content/drive/My Drive/MUIC/bigData/shakespear.txt')
'''

dataRDD = sc.textFile('gs://spark-prac-pakin/shakespear.txt')

lowerRDD = dataRDD.map(lambda line:line.lower())

print(lowerRDD.glom().collect())#show the RDD

wordRDD = lowerRDD.flatMap(lambda line:line.split(" "))

#remove special char

import re
def clean_word(word):
  res = re.sub(r'[^a-zA-Z0-9]','',word)
  if res:
    return (res,1)

cleanRDD = wordRDD.map(clean_word)
cleanRDD = cleanRDD.filter(lambda x : x is not None and x != ('', 1)) # Filter out None and empty string tuples

wordCountRDD = cleanRDD.reduceByKey(lambda a,b : (a+b))

sort = wordCountRDD.sortByKey()

print(sort.glom().collect())

sort.saveAsTextFile("gs://spark-prac-pakin/output/")